\documentclass[usenames,dvipsnames,mathserif,compress]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}

\useoutertheme{miniframes}
\usecolortheme{dolphin}

\lstset{%,
  basicstyle=\small\ttfamily,
  keywordstyle=\color{red},
  identifierstyle=\color{Blue},
  stringstyle=\color{Green},
  commentstyle=\ttfamily\color{OliveGreen},
  showstringspaces=false,
  language=C}%,


\author{Ask Hjorth Larsen}
\title{Introduction to high-performance computing}
\institute{Nano-bio Spectroscopy Group and ETSF Scientific Development Centre,\\
Universidad del País Vasco UPV/EHU, San Sebastián, Spain}

\begin{document}
%  hello world

\begin{frame}
  \maketitle
\end{frame}

\section*{Introduction}
\subsection*{}

\begin{frame}
  \begin{block}{The CPU}
    \begin{itemize}
    \item The CPU reads \alert{instructions and inputs}, then performs those instructions on the inputs
    \item Instruction codes and inputs are processed in workspaces
      on the CPU called \alert{registers}
    \item Each cycle, the CPU can execute an instruction; different
      CPU architectures support different instructions
    \end{itemize}
  \end{block}
  \begin{block}{Example}
    \begin{itemize}
    \item Retrieve number from address A, put it in register R
    \item Add numbers from registers R and R', store sum in R''
    \item Write number from R'' to address A'
    \item Etc.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{HPC basics}
  \begin{itemize}
  \item Most time is probably spent with floating point operations
  \item Important: Retrieve data from memory efficiently
  \end{itemize}
  \end{block}
  \begin{block}{Some programming languages}
    \begin{itemize}
    \item (Assembly language)
    \item Fortran (1957, 1977, 1995, 2003, \ldots)
    \item C/C++
    \item Python --- C extensions, Numpy, Scipy, \ldots
    \end{itemize}
  \end{block}
\end{frame}

\section*{Number crunching}
\subsection*{Pipelining}

\begin{frame}
  \frametitle{Floating point numbers}
  Computational physics mostly boils down to multiplying floating point numbers.
  \begin{block}{IEEE 754 standard for floating point numbers}
    \begin{itemize}
    \item Number is represented as $M \times 2^n$
    \item $M$ is the significand or mantissa
    \item $n$ is the exponent
    \end{itemize}
  \end{block}
  \begin{block}{Important types}
    \begin{itemize}
    \item 32-bit single precision: 24 bit for $M$, 8 for $n$
    \item 64-bit double precision: 53 bit for $M$, 11 for $n$
    \end{itemize}
  \end{block}
  Floating point operations are complex.
  Modern CPUs have one or more \alert{floating point units} (FPUs)
  that execute floating point operations efficiently
\end{frame}


\begin{frame}
  \frametitle{Pipelining}
  \begin{itemize}
  \item Consider a 4-step operation where different ``units'' can process
    different steps simultaneously
    \begin{table}
    \begin{tabular}{c|cccccccccccc}
      & u1 &u2&u3&u4\\\hline
      Cycle 1 & A$^1$ &ø&ø&ø \\
      Cycle 2 & B$^1$ & A$^2$ &ø&ø \\
      Cycle 3 & C$^1$ & B$^2$ & A$^3$ &ø \\
      Cycle 4 & D$^1$ & C$^2$ & B$^3$ & A$^4$  \\
      Cycle 5 & E$^1$ & D$^2$ & C$^3$ & B$^4$   \\
      Cycle 6 & F$^1$ & E$^2$ & D$^3$ & C$^4$  \\
      Cycle 7 & ø     & F$^2$ & E$^3$ & D$^4$  \\
      Cycle 8 & ø & ø & F$^3$ & E$^4$  \\
      Cycle 9 & ø & ø & ø & F$^4$  \\
    \end{tabular}
    \end{table}
  \item Can execute up to one whole operation per cycle, but cycles
    may be wasted flushing/filling the pipeline
  \item Next input element must be readily available
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Branching and pipelining}
  \begin{itemize}
  \item Jumping around in memory breaks the pipeline
  \item Avoid \alert{branching} in high-performance loops: \texttt{if} statements, function calls, \texttt{goto}, \ldots
  \item Jump can be eliminated by \alert{inlining} --- include the source of a function ``inline'' in place of calling the function, e.g.\ using a macro
  \item Also: \lstinline{inline double myfunction(...)}
  \end{itemize}
\end{frame}


\subsection*{Caching}

\begin{frame}
  \frametitle{Memory and multilevel caching}
  Example: Intel i7-4770 Haswell architecture\\
  \medskip
  \begin{tabular}{llll}
     & Size & Latency & Total \\
    \hline\hline
    L1 cache & 64 KB/core & 4--5 cycles & 1.3 ns\\
    L2 cache & 256 KB/core & 12 cycles & 3.5 ns \\
    L3 cache & 8 MB, shared & 36 cycles & 11 ns\\
    Main memory & 32 GB, shared & $36\textsf{ cycles} + 57$ ns & 68 ns\\\hline\hline
  \end{tabular}
  Source: \url{http://www.7-cpu.com/cpu/Haswell.html}
  \bigskip
  \begin{itemize}
  \item When accessing memory, contiguous chunks of adjacent memory will be copied into cache
  \item A failed cache lookup is called a ``cache miss''
  \item Upon cache miss, element is looked up at next (slower) level
  \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Arrays and memory layout}
  \begin{itemize}
  \item  Standard mathematical matrix notation:
  \begin{align}
    \left[
  \begin{matrix}
    1,1 & 1,2 & 1,3 \\
    2,1 & 2,2 & 2,3 \\
    3,1 & 3,2 & 3,3
  \end{matrix}
  \right]\nonumber
  \end{align}
\item Elements of the array are stored in a \alert{contiguous} chunk of memory,
  but the ordering depends on language
\item Fortran memory order is \alert{column-major}:
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
     $1,1$&$2,1$&$3,1$&$1,2$&$2,2$&$3,2$&$1,3$&$2,3$&$3,3$ \\\hline
  \end{tabular}
\item C memory order is \alert{row-major}:
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
     $1,1$&$1,2$&$1,3$&$2,1$&$2,2$&$2,3$&$3,1$&$3,2$&$3,3$ \\\hline
  \end{tabular}
\item Accessing elements in memory order is fast.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing cache use}
  \begin{itemize}
  \item Work on contiguous chunks of memory
  \end{itemize}
\begin{lstlisting}
// fast
for(i=0; i < I; i++) {
  for(j=0; j < J; j++) {
    a[i * J + j] = ...
  }
}

// Slow
for(j=0; j < J; j++) {
  for(i=0; i < I; i++) {
    a[i * J + j] = ...
  }
}
\end{lstlisting}
\end{frame}

\subsection*{Matrix multiplication}

\begin{frame}[fragile]
  \frametitle{Benchmark}
  \begin{align}
    \hspace{-2cm} \textrm{Matrix multiplication}\quad c_{ij} &= \sum_k a_{ik} b_{kj}\nonumber
  \end{align}
\begin{lstlisting}
void matmul_ikj(int I, int J, int K,
                double *A, double *B, double *C)
{
  int i, j, k;
  for(i=0; i < I; i++) {
    for(k=0; k < K; k++) {
      for(j=0; j < J; j++) {
        C[i * J + j] += A[i * K + k] * B[k * J + j];
      }
    }
  }
}
\end{lstlisting}
Different permutations of $\{ikj\}$ loops will perform differently
\end{frame}


\begin{frame}
  \begin{figure}
  \includegraphics[width=.95\textwidth]{code/timings-matmul}
  \caption{Timings for matrix multiplication
  at \texttt{-O2} optimization level}
  \end{figure}
\end{frame}

% Part 1: HPC
%  * floating point operations
%  * pipelining
%  * cache levels
%  * loop unrolling
%  * matmul
%  * BLAS

\subsection*{More optimizations}

\begin{frame}[fragile]
\frametitle{Loop unrolling}
\begin{itemize}
\item Unrolling eliminates a fraction of loop bounds checks.
\begin{lstlisting}
for(i=0; i < 4; i++) {
    a[i] = b[i] * c[i];
}
\end{lstlisting}
\item Unrolled:
\begin{lstlisting}
a[i] = b[i] * c[i];
a[i + 1] = b[i + 1] * c[i + 1];
a[i + 2] = b[i + 2] * c[i + 2];
a[i + 3] = b[i + 3] * c[i + 3];
\end{lstlisting}
\item Compiler may be able to unroll automatically (e.g.~\texttt{-funroll-loops}).
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Blocking}
  \begin{itemize}
  \item
  Compute $\mathbf C = \mathbf A \mathbf B$ where each matrix is composed into
  blocks:
  \begin{align}
    \mathbf A = \left[
      \begin{matrix}
        \mathbf A_{11} &\cdots& \mathbf A_{1n}\\
        \vdots & & \vdots \\
        \mathbf A_{n1} & \cdots & \mathbf A_{nn}
      \end{matrix}\right]\nonumber
  \end{align}

\item Matrix product expressed with blocks:
  \begin{align}
    \mathbf C_{ij} = \sum_{k} \mathbf A_{ik} \mathbf B_{kj}\nonumber
  \end{align}
  \item Work on smaller blocks that fit into cache
  \item Optimal blocksize depends on architecture (e.g.\ cache size)
  \item Matrix product scales as $\mathcal O(N^3)$
  \item Blocking improves $\mathcal O(N^3)$ prefactor by working on
    chunks that fit in cache
  \end{itemize}
\end{frame}

\subsection*{BLAS and LAPACK}

\begin{frame}
  \frametitle{BLAS}
  \begin{block}{Basic Linear Algebra Subprograms}
  \end{block}
  \begin{itemize}
  \item Standard interface for standard operations:
    Matrix multiplication
  \item Highly optimized for different platforms individually
  \end{itemize}
  \begin{block}{Some BLAS implementations}
    \begin{itemize}
    \item RefBlas --- reference implementation from Netlib
    \item OpenBlas (based on older GotoBlas)
    \item Atlas --- automatically tuned linear algebra software
    \item Intel MKL
    \item AMD ACML
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Some BLAS functions}
    \begin{itemize}
    \item \texttt{dgemm}: \alert double-precision \alert{ge}neral \alert matrix--\alert matrix multiply
    \item \texttt{dsymv}: \alert double-precision \alert{sy}mmetric \alert matrix--\alert vector multiply
    \item \texttt{daxpy}: \alert double-precision $\alert{a \mathbf X}\textrm{ \alert plus } \alert y$
    \item \texttt{zsyr2k}: ``complex double-precision (\alert z) \alert{sy}mmetric \alert ran\alert{k-2} update'',
      $\mathbf X \mathbf Y^T + \mathbf Y \mathbf X^T$
    \item Etc.
    \end{itemize}
  \end{block}
  \begin{block}{LAPACK: Linear Algebra PACKage}
    \begin{itemize}
    \item Higher-level linear algebra operations
    \item LU-decomposition, eigenvalues, \ldots
    \item \texttt{dsyev}: \alert double-precision \alert{sy}mmetric \alert eigen{\alert v}alues
    \item Etc.
    \end{itemize}
  \end{block}
  For best performance, use BLAS/LAPACK whenever possible
\end{frame}

\begin{frame}
  \frametitle{Simple matrix multiplication vs BLAS}
  \begin{figure}
    \includegraphics[width=.95\textwidth]{code/timings-matmul-blas.pdf}
  \end{figure}
\end{frame}

% Part 2: Parallelization
%  * threading
%  * MPI/distributed-memory
%  * Amdahl's law etc.
%  * Discussion of efficiency
%  * Examples?  Redist-calculate-redist
%  * ScaLAPACK

\section*{Parallelization}
\subsection*{Parallel programs}


\begin{frame}
  %\frametitle{Parallel programs}
  \begin{block}{Shared memory}
    \begin{itemize}
    \item Multiple threads work simulaneously, access same variables
    \item Threads may read the same memory simultaneously, but
      simultaneous writing leads to \alert{race condition}
    \item Threads must therefore \alert{synchronize} access to the memory
      (e.g.~\texttt{synchronized} methods and blocks in Java)
    \item Synchronize means: ``Lock, run, unlock''
    \end{itemize}
  \end{block}
  \begin{block}{Distributed memory}
    \begin{itemize}
    \item Each process has its own chunk of memory, probably on different physical computers
    \item No problem with synchronizing memory (unless also threading)
    \item Must manually send/receive all data; much more difficult
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Parallel pitfalls and deadlocks}
  \begin{block}{Example 1}
    \begin{itemize}
    \item Process 1 sends 5 numbers to process 2
    \item Process 1 expects something from process 2
    \item Process 2 expects 6 numbers from process 1, receives 5
    \item Both processes now wait forever
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Parallel pitfalls and deadlocks}
  \begin{block}{Example 2}
    \begin{itemize}
    \item Process A reserves Sala Capitular for this talk
    \item Process B attempts to reserve Sala Capitular for this talk,
      but Sala Capitular is already reserved for some talk
    \item Process B schedules this talk for another lecture room
    \item \ldots
  %\item Resource A is blocked by process N
  %\item Process N waits for resource B
  %\item Resource B is blocked by process M
  %\item Process M waits for resource A
  %\item Process A expects 10 bytes from process B
  %\item Process B sends only 5 bytes
  %\item Process A holds resource X, waits for resource Y
  %\item Process B holds resource Y, waits for resource X
  \end{itemize}
  \end{block}
\end{frame}

\subsection*{Parallel programming}

\begin{frame}[fragile]
  \frametitle{MPI --- Message Parsing Interface}
  \begin{itemize}
  \item Programming interface specification for distributed-memory parallelization
  \item Implementations: OpenMPI, MPICH, \ldots
  \item \alert{Communicator}: Object which represents a group of processes that may communicate amongst themselves
  \item \verb#MPI_COMM_WORLD# --- the communicator of all processes
  \item The \alert{size} of a communicator is how many processes participate
  \item Each process has a \alert{rank} within the communicator: 0, 1, 2, \ldots, size.
  \item Run on 8 cores: \texttt{mpirun -np 8 myprogram}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Parallel hello world}
  \lstinputlisting[basicstyle=\scriptsize\ttfamily]{mpi/mpihello.c}
\end{frame}

\begin{frame}[fragile]
  \begin{columns}
    \begin{column}{1.05\textwidth}
  \lstinputlisting[firstline=4,basicstyle=\scriptsize\ttfamily]{mpi/mpihello2.c}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{BLACS/ScaLAPACK}
  \begin{itemize}
  \item BLACS: Basic Linear Algebra Communication Subprograms
  \item ScaLAPACK: Like LAPACK, but in parallel
  \item BLACS uses ``2D block-cyclic memory layout'': Processes are arranged in a 2D grid, arrays are distributed in blocks
  \item Distribution of blocks among ranks:
    \begin{align}
      \begin{array}{|c|c|c|c|c|c|}
        \hline
          0 & 1 & 2 & 0 & 1 & 2\\\hline
          3 & 5 & 5 & 3 & 4 & 5\\\hline
          0 & 1 & 2 & 0 & 1 & 2\\\hline
          3 & 5 & 5 & 3 & 4 & 5\\\hline
        \end{array}\nonumber
    \end{align}
  \item \texttt{pdgemm}, \texttt{pdsyev}, \ldots
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Parallel scaling}
  \begin{columns}
    \begin{column}{0.67\textwidth}
        \begin{block}{GPAW/LCAO performance}
      \begin{itemize}
      \item Time per iteration, 2500--10k atoms
      \item ``Strong-scaling'' test: Fixed problem size, increase CPU count
      \item More processes increase speed, but also overhead
      \item More processes may be necessary when calculation does not fit into memory
      \end{itemize}
        \end{block}
    \end{column}
    \begin{column}{.4\textwidth}
      \includegraphics[scale=0.85]{lcao-scaling-crop.pdf}
    \end{column}
  \end{columns}
\end{frame}


\subsection*{Supercomputing}

%\begin{frame}
%  \frametitle{Supercomputers}
%  \begin{itemize}
%  \item ``Everyday'' supercomputers (like Quinde here) consist of many nodes, each with typically 4--32 cores, connected by a high-performance network
%  \item Threading can be used within a core, but most (electronic structure)
%    applications will use multiple cores with MPI
%  \item Computers are typically accessed through a login node
%  \item Users submit jobs to queues
%  \item Computers may have GPUs
%  \end{itemize}
%\end{frame}

\begin{frame}
  \frametitle{Quinde supercomputer at Yachay}
  \begin{itemize}
  \item 84 compute nodes, dual Power-8 10-core CPUs (20 cores/node)
  \item Dual NVidia K-80 graphics cards, 8 tera-FLOPS
  \item 128 GB memory per node
  \item Interconnect: Mellanox 100 Gbit InfiniBand
  \end{itemize}
  \vspace{-5mm}
  \begin{figure}
  \includegraphics[width=0.7\textwidth]{quinde.jpg}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Massively parallel architectures}
  \begin{itemize}
  \item Hundreds of thousands of cores, very scalable
  \item High demands on interconnect: Network topology, locality
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.73\textwidth]{IBM_Blue_Gene_P_supercomputer.jpg}
    \caption{IBM BlueGene/P (Image source: Wikipedia)}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{GPUs --- graphics cards for computing}
  \begin{itemize}
  \item A graphics card is a shared-memory processor running a very large number of threads
  \item Graphics cards are the cheapest way of multiplying many floating point numbers
  \item Special architecture: Code must be explicitly written for graphics cards
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{GPU performance}
  \begin{itemize}
  \item On a normal processor, each thread (a, b, c) should work on a contiguous piece of data (1, 2, 3):
    \begin{table}
    \begin{tabular}{|ccc|ccc|ccc|}\hline
      a1 &a2 &a3 &b1 &b2 &b3 &c1 &c2 &c3\\\hline
    \end{tabular}
    \end{table}
  \item On a graphics card, memory bandwidth (main memory to graphics card memory) is critical
  \item Threads a, b, and c can start quickly only with a \alert{strided} memory layout:
    \begin{table}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}\hline
      a1 &b1 &c1 &a2 &b2 &c2 &a3 &b3 &c3\\\hline
    \end{tabular}
    \end{table}
  \item Here, threads a, b, c will all run once we have received three chunks
  \item In the previous case, b and c would still be idle after receiving three chunks
  \end{itemize}
\end{frame}

\section*{Conclusions}
\subsection*{}
\begin{frame}
  \frametitle{Summary \& concluding remarks}
  \begin{itemize}
  \item Pipelining, memory locality
  \item Parallelization: Threading, MPI
  \item HPC libraries: BLAS, LAPACK, ScaLAPACK
  \end{itemize}
\end{frame}


% Part 3: Supercomputers
%  * Normal clusters
%  * BlueGene/P
%  * Graphics cards


% Something about background
%  Running electronic structure calculations economically
%  Getting your results faster

% How computers work
% What a processor does
% Memory
% Pointers
% etc.


% Matrix multiplication example

% Parallelization
% Amdahl's law, maybe also the other one about weak scaling
% Embarrassingly parallel; communication bottlenecks etc.
% Shared memory: Threading, OpenMP
% Distributed memory: MPI

% BLAS, LAPACK
% ScaLAPACK

% Scripting: Python, numpy, scipy

\end{document}
